{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# The New York Social Graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[New York Social Diary](https://www.newyorksocialdiary.com/) provides a\n",
    "fascinating lens onto New York's socially well-to-do.  The data forms a natural social graph for New York's social elite.  Take a look at this page of a recent [To Love Unconditionally](https://www.newyorksocialdiary.com/to-love-unconditionally/). \n",
    "\n",
    "You will notice the photos have carefully annotated captions labeling those that appear in the photos.  We can think of this as implicitly implying a social graph: there is a connection between two individuals if they appear in a picture together.\n",
    "\n",
    "For this project, we will assemble the social graph from photo captions for parties.  Using this graph, we can make guesses at the most popular socialites, the most influential people, and the most tightly coupled pairs.\n",
    "\n",
    "We will attack the project in three phases:\n",
    "1. Get a list of all the photo pages to be analyzed. \n",
    "2. Get all captions in each party, Parse all of the captions and extract guests' names.\n",
    "3. Assemble the graph, analyze the graph and answer the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Phase One\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The first step is to crawl the data.  We want photos from parties on or before December 1st, 2014.  Go to the [Party Pictures Archive](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures) to see a list of (party) pages.  We want to get the url for each party page, along with its date.\n",
    "\n",
    "Here are some packages that you may find useful.  You are welcome to use others, if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import dill\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We recommend using Python [Requests](http://docs.python-requests.org/en/master/) to download the HTML pages, and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) to process the HTML.  Let's start by getting the [first page](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "url = \"https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures\"\n",
    "page = requests.get(url) # Use requests.get to download the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, we process the text of the page with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Fetch the HTML content\n",
    "url = 'http://example.com'  # Replace with your target URL\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Step 2: Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the specific <div> (example using class)\n",
    "div = soup.find('div', class_='specific-class')  # Replace 'specific-class' with the actual class name\n",
    "\n",
    "# Step 4: Extract the link from the <div>\n",
    "if div:\n",
    "    link = div.find('a')  # Find the first <a> tag within the <div>\n",
    "    if link and 'href' in link.attrs:\n",
    "        href = link['href']\n",
    "        print('Link:', href)\n",
    "    else:\n",
    "        print('No link found in the specified <div>.')\n",
    "else:\n",
    "    print('No <div> found with the specified class.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "This page has links to 50 party pages. Look at the structure of the page and determine how to isolate those links.  Your browser's developer tools (usually `Cmd`-`Option`-`I` on Mac, `Ctrl`-`Shift`-`I` on others) offer helpful tools to explore the structure of the HTML page.\n",
    "\n",
    "Once you have found a pattern, use BeautifulSoup's [select](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#css-selectors) or [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find) methods to get those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/bunny-hop-the-boys-club-old-bags-and-more', (2015, 3, 13)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/sab-the-jewish-museum-roundabout-theatre-and-faces', (2015, 3, 11)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/adaa-art-show-bronx-museum-the-china-arts-foundation-international-and-the-palm', (2015, 3, 5)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/the-60th-anniversary-of-the-viennese-opera-ball-and-mcnys-directors-council', (2015, 3, 4)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/the-new-york-philharmonic-the-new-york-botanical-garden-longhouse-reserve-and', (2015, 3, 2)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/mission-accomplished', (2015, 2, 26)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/philanthropic-endeavors', (2015, 2, 25)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/dining-with-the-divas', (2015, 2, 23)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/fielding-dreams', (2015, 2, 18)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/generosity-leadership', (2015, 2, 11)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/young-new-yorkers', (2015, 2, 9)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/preservation-and-care', (2015, 2, 6)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/young-collectors-and-benefactors', (2015, 2, 5)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/a-public-service', (2015, 2, 2)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/adventures-to-and-from', (2015, 1, 30)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/was-and-abt', (2015, 1, 26)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/starlets-designers-and-art-lovers', (2015, 1, 23)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/great-expectations', (2015, 1, 21)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/new-york-to-palm-beach-and-back-again', (2015, 1, 14)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/from-the-garden-conservatory-to-the-beach-bash', (2015, 1, 12)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/expertise-and-excellence', (2015, 1, 2)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/with-champagne-a-filling', (2014, 12, 29)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/high-above-their-heads', (2014, 12, 22)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/holiday-cheer', (2014, 12, 19)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/winter-wonderland-ball', (2014, 12, 18)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers', (2014, 12, 17)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/family-fundraising-events', (2014, 12, 15)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/family-gatherings', (2014, 12, 11)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/expert-eye', (2014, 12, 8)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/every-day-is-a-holiday', (2014, 12, 3)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/the-thanksgiving-day-parade-from-the-ground-up', (2014, 12, 1)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/gala-guests', (2014, 11, 24)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/equal-justice', (2014, 11, 20)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/celebrating-the-treasures', (2014, 11, 18)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/associates-and-friends', (2014, 11, 17)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/michaels-25th', (2014, 11, 13)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/new-york-lifelines', (2014, 11, 12)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/legends-and-leaders', (2014, 11, 10)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/fall-fun', (2014, 11, 6)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/park-avenue-armorys-2014-gala-masquerade', (2014, 11, 5)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/old-friends-new-friends', (2014, 11, 3)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/new-york-new-york-a-helluva-town', (2014, 10, 30)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/patrons-sponsors-supporters-friends-alumni-and-members', (2014, 10, 27)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/the-fricks-annual-autumn-dinner-and-casita-marias-2014-fiesta', (2014, 10, 23)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/new-york-and-beyond', (2014, 10, 22)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/compassion-choices', (2014, 10, 17)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/public-celebrations', (2014, 10, 13)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/junior-councils-taking-the-lead', (2014, 10, 8)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/luncheons-lectures-and-masked-balls', (2014, 10, 6)), ('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2014/opening-numbers', (2014, 10, 2))]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "def filter_by_date(links, cutoff=datetime(2014, 12, 1)):\n",
    "    filtered = []\n",
    "    print(len(links))\n",
    "    for link in links:\n",
    "        url, date = link\n",
    "        \n",
    "        # Check if the URL and date exist and date is valid\n",
    "        if date and url and (date[0] < cutoff.year or (date[0] <= cutoff.year and date[1] < cutoff.month) or (date[0] == cutoff.year and date[1] == cutoff.month and date[2] == cutoff.day)):\n",
    "            filtered.append(link)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def convert_date(date_str):\n",
    "    try:\n",
    "        input_format = \"%A, %B %d, %Y\"\n",
    "        date_obj = datetime.strptime(date_str, input_format)\n",
    "        return date_obj.year, date_obj.month, date_obj.day\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting date '{date_str}': {e}\")\n",
    "        return None\n",
    "    \n",
    "def get_link_date(link):\n",
    "    try:\n",
    "        # Extract the URL from the title field\n",
    "        titleField = link.find(class_=\"field-content\")\n",
    "        url = titleField.find(\"a\")['href'] if titleField and titleField.find(\"a\") else None\n",
    "        pattern = r'^.*?(http)'\n",
    "        cleaned_url = re.sub(pattern, r'\\1', url) if url else None\n",
    "        \n",
    "        # Extract the date from the created field\n",
    "        dateField = link.find(class_=\"views-field-created\")\n",
    "        date_str = dateField.find(class_=\"field-content\").text if dateField else None\n",
    "        \n",
    "        # Convert the date string to a tuple (year, month, day)\n",
    "        date = convert_date(date_str) if date_str else None\n",
    "        \n",
    "        return (url, date)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing link: {e}\")\n",
    "        return (None, None)\n",
    "def get_page_links(page_link):\n",
    "    try:\n",
    "        page = requests.get(page_link)\n",
    "        soup = BeautifulSoup(page.text, \"lxml\")\n",
    "        \n",
    "        # Find all rows containing links and dates\n",
    "        rows = soup.find_all(class_=\"views-row\")\n",
    "        \n",
    "        # Store the (url, date) tuples\n",
    "        links_and_dates = [get_link_date(row) for row in rows]\n",
    "        \n",
    "        return links_and_dates\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching page {page_link}: {e}\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "links = get_page_links(\"https://web.archive.org/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures?page=1\")\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There should be 50 per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links) == 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's take a look at that first link.  Figure out how to extract the URL of the link, as well as the date.  You probably want to use `datetime.strptime`.  See the [format codes for dates](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2015/grand-finale-of-the-hampton-classic-horse-show\">Grand Finale of the Hampton Classic Horse Show</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/web/20150918040703/http://www.newyorksocialdiary.com/party-pictures/2015/bunny-hop-the-boys-club-old-bags-and-more',\n",
       " (2015, 3, 13))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = links[0]\n",
    "# Check that the title and date match what you see visually.\n",
    "link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "For purposes of code reuse, let's put that logic into a function.  It should take the link element and return the URL and date parsed from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def get_link_date(el):\n",
    "    ...\n",
    "    return url, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You may want to check that it works as you expected.\n",
    "\n",
    "Once that's working, let's write another function to parse all of the links on a page.  Thinking ahead, we can make it take a Requests [Response](https://requests.readthedocs.io/en/master/api/#requests.Response) object and do the BeautifulSoup parsing within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def get_links(response):\n",
    "    return ... # A list of URL, date pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "If we run this on the previous response, we should get 50 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# These should be the same links from earlier\n",
    "len(get_links(page)) == 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "But we only want parties with dates on or before the first of December, 2014.  Let's write a function to filter our list of dates to those at or before a cutoff.  Using a keyword argument, we can put in a default cutoff, but allow us to test with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def filter_by_date(links, cutoff=datetime(2014, 12, 1)):\n",
    "    # Return only the elements with date <= cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "With the default cutoff, there should be no valid parties on the first page.  Adjust the cutoff date to check that it is actually working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# Double check the dates are being extracted correctly\n",
    "len(filter_by_date(get_links(page))) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now we should be ready to get all of the party URLs.  Click through a few of the index pages to determine how the URL changes.  Figure out a strategy to visit all of them.\n",
    "\n",
    "HTTP requests are generally IO-bound.  This means that most of the time is spent waiting for the remote server to respond.  If you use `requests` directly, you can only wait on one response at a time.  [requests-futures](https://github.com/ross/requests-futures) lets you wait for multiple requests at a time.  You may wish to use this to speed up the downloading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "link_list = []\n",
    "# You can use link_list.extend(others) to add the elements of others\n",
    "# to link_list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In the end, you should have 1193 parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# Make sure you are using the same /web/stringofdigits/... for each page\n",
    "# This is to prevent the archive from accessing later copies of the same page\n",
    "# If you are off by a just a few, that can be the archive misbehaving\n",
    "len(link_list) == 1193"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In case we need to restart the notebook, we should save this information to a file.  There are many ways you could do this; here's one using `dill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "dill.dump(link_list, open('nysd-links.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "To restore the list, we can just load it from the file.  When the notebook is restarted, you can skip the code above and just run this command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "link_list = dill.load(open('nysd-links.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "### Question 1: In which month did most of the parties occur? (10 p)\n",
    "### Question 2: What is the overall trend of parties from 2007 to 2014? (10 p)\n",
    "Use visualizations to answer the two questions above. Ensure that you interpret your plots thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Phase Two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "In this phase, we concentrate on getting the names out of captions for a given page.  We'll start with [the benefit cocktails and dinner](https://web.archive.org/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood) for [Lenox Hill Neighborhood House](http://www.lenoxhill.org/), a neighborhood organization for the East Side.\n",
    "\n",
    "Take a look at that page.  Note that some of the text on the page is captions, but others are descriptions of the event.  Determine how to select only the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "captions = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "By our count, there are about 110.  But if you're off by a couple, you're probably okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# These are for the specific party referenced in the text\n",
    "abs(len(captions) - 110) < 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's encapsulate this in a function.  As with the links pages, we want to avoid downloading a given page the next time we need to run the notebook.  While we could save the files by hand, as we did before, a checkpoint library like [ediblepickle](https://pypi.python.org/pypi/ediblepickle/1.1.3) can handle this for you.  (Note, though, that you may not want to enable this until you are sure that your function is working.)\n",
    "\n",
    "You should also keep in mind that HTTP requests fail occasionally, for transient reasons.  You should plan how to detect and react to these failures.   The [retrying module](https://pypi.python.org/pypi/retrying) is one way to deal with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "def get_captions(path):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "This should get the same captions as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# This cell is expecting get_captions to return a list of the captions themselves\n",
    "# Other routes to a solution might need to adjust this cell a bit\n",
    "captions == get_captions(\"/web/20150913224145/http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now that we have some sample captions, let's start parsing names out of those captions.  There are many ways of going about this, and we leave the details up to you.  Some issues to consider:\n",
    "\n",
    "  1. Some captions are not useful: they contain long narrative texts that explain the event.  Try to find some heuristic rules to separate captions that are a list of names from those that are not.  A few heuristics include:\n",
    "    - look for sentences (which have verbs) and as opposed to lists of nouns. For example, [`nltk` does part of speech tagging](http://www.nltk.org/book/ch05.html) but it is a little slow. There may also be heuristics that accomplish the same thing.\n",
    "    - Similarly, spaCy's [entity recognition](https://spacy.io/docs/usage/entity-recognition) could be useful here.\n",
    "    - Look for commonly repeated threads (e.g. you might end up picking up the photo credits or people such as \"a friend\").\n",
    "    - Long captions are often not lists of people.  The cutoff is subjective, but for grading purposes, *set that cutoff at 250 characters*.\n",
    "  2. You will want to separate the captions based on various forms of punctuation.  Try using `re.split`, which is more sophisticated than `string.split`. **Note**: Use regex exclusively for name parsing.\n",
    "  3. This site is pretty formal and likes to say things like \"Mayor Michael Bloomberg\" after his election but \"Michael Bloomberg\" before his election.  Can you find other titles that are being used?  They should probably be filtered out because they ultimately refer to the same person: \"Michael Bloomberg.\"\n",
    "  4. There is a special case you might find where couples are written as eg. \"John and Mary Smith\". You will need to write some extra logic to make sure this properly parses to two names: \"John Smith\" and \"Mary Smith\".\n",
    "  5. When parsing names from captions, it can help to look at your output frequently and address the problems that you see coming up, iterating until you have a list that looks reasonable. Because we can only asymptotically approach perfect identification and entity matching, we have to stop somewhere.\n",
    "  \n",
    "**Questions worth considering:**\n",
    "  1. Who is Patrick McMullan and should he be included in the results? How would you address this?\n",
    "  2. What else could you do to improve the quality of the graph's information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once you feel that your algorithm is working well on these captions, parse all of the captions and extract all the names mentioned.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, run this sort of test on a few other pages.  You will probably find that other pages have a slightly different HTML structure, as well as new captions that trip up your caption parser.  But don't worry if the parser isn't perfect -- just try to get the easy cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once you are satisfied that your caption scraper and parser are working, run this for all of the pages.  If you haven't implemented some caching of the captions, you probably want to do this first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "# Scraping all of the pages could take 10 minutes to an hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Graph Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "For the remaining analysis, we think of the problem in terms of a\n",
    "[network](http://en.wikipedia.org/wiki/Computer_network) or a\n",
    "[graph](https://en.wikipedia.org/wiki/Graph_%28discrete_mathematics%29).  Any time a pair of people appear in a photo together, that is considered a link.  It is an example of  an **undirected weighted graph**. We recommend using python's [`networkx`](https://networkx.github.io/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import itertools  # itertools.combinations may be useful\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "All in all, you should end up with over 100,000 captions and more than 110,000 names, connected in about 200,000 pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 3: Graph EDA (20 p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use parsed names to create the undirected weighted network and visualize it (5 p)\n",
    "- Report the number of nodes and edges (5 p)\n",
    "- What is the diameter of this graph? (5 p)\n",
    "- What is the average clustering coeff of the graph? How you interpret this number? (5 p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Graph properties (20 p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What real-world graph properties does this graph exhibit? Please show your work and interpret your answer. Does the result make sense given the nature of the graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Who are the most photogenic persons? (10 p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The simplest question to ask is \"who is the most popular\"?  The easiest way to answer this question is to look at how many connections everyone has.  Return the top 100 people and their degree.  Remember that if an edge of the graph has weight 2, it counts for 2 in the degree.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution\n",
    "\n",
    "    \"count\": 100.0\n",
    "    \"mean\": 189.92\n",
    "    \"std\": 87.8053034454\n",
    "    \"min\": 124.0\n",
    "    \"25%\": 138.0\n",
    "    \"50%\": 157.0\n",
    "    \"75%\": 195.0\n",
    "    \"max\": 666.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "degree = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 5: Centrality analysis (20 p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Use eccentricity centrality, closeness centrality, betweenness centrality, prestige, and PageRank to identify the top 10 individuals with the highest centrality for each measure. How do you interpret the results?\n",
    "\n",
    "Use 0.85 as the damping parameter for page rank, so that there is a 15% chance of jumping to another vertex at random.\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution for pagerank\n",
    "\n",
    "    \"count\": 100.0\n",
    "    \"mean\": 0.0001841088\n",
    "    \"std\": 0.0000758068\n",
    "    \"min\": 0.0001238355\n",
    "    \"25%\": 0.0001415028\n",
    "    \"50%\": 0.0001616183\n",
    "    \"75%\": 0.0001972663\n",
    "    \"max\": 0.0006085816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 6: best_friends (10 p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Another interesting question is who tend to co-occur with each other.  Give us the 100 edges with the highest weights.\n",
    "\n",
    "Google these people and see what their connection is.  Can we use this to detect instances of infidelity?\n",
    "\n",
    "**Checkpoint:** Some aggregate stats on the solution\n",
    "\n",
    "    \"count\": 100.0\n",
    "    \"mean\": 25.84\n",
    "    \"std\": 16.0395470855\n",
    "    \"min\": 14.0\n",
    "    \"25%\": 16.0\n",
    "    \"50%\": 19.0\n",
    "    \"75%\": 29.25\n",
    "    \"max\": 109.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "#temple form for the answer\n",
    "best_friends = [(('Michael Kennedy', 'Eleanora Kennedy'), 41)] * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
